"""
Research stages and prompts for AI Research Assistant
"""

import asyncio
import httpx
from typing import Dict, List, Any
import os
import json
import re
import pdfplumber
import json
from datetime import datetime

class ResearchStage:
    """Base class for research stages"""
    
    def __init__(self, name: str, description: str, icon: str):
        self.name = name
        self.description = description
        self.icon = icon
        self.status = "pending"  # pending, active, completed, error
        self.progress = 0
        self.result = None
        self.error = None

class ResearchProcessor:
    """Main processor for research stages"""
    
    def __init__(self, config, manager, client_id: str):
        self.config = config
        self.manager = manager
        self.client_id = client_id
        self.stages = []
        self.current_stage = 0
        
    async def send_update(self, stage_name: str, status: str, progress: int, message: str = ""):
        """Send update to client via WebSocket"""
        await self.manager.send_message(self.client_id, {
            "type": "stage_update",
            "stage": stage_name,
            "status": status,
            "progress": progress,
            "message": message,
            "timestamp": datetime.now().isoformat()
        })
    
    async def _execute_with_retry(self, func, *args, stage_name: str, stage_description: str, max_retries: int = 3):
        """Execute function with retry mechanism"""
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –¥–ª—è {stage_description}")
                
                if attempt > 0:
                    await self.send_update(stage_name, "active", 0, f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}...")
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff: 2, 4, 8 seconds
                
                result = await func(*args)
                print(f"‚úÖ {stage_description} —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt + 1}")
                return result
                
            except (httpx.TimeoutException, httpx.ReadTimeout, httpx.ConnectTimeout) as e:
                last_exception = e
                print(f"‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}: {str(e)}")
                if attempt < max_retries - 1:
                    await self.send_update(stage_name, "active", 0, f"–¢–∞–π–º–∞—É—Ç, –ø–æ–≤—Ç–æ—Ä—è–µ–º —á–µ—Ä–µ–∑ {2 ** (attempt + 1)} —Å–µ–∫...")
                else:
                    await self.send_update(stage_name, "error", 0, f"–¢–∞–π–º–∞—É—Ç –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    
            except Exception as e:
                last_exception = e
                print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}: {str(e)}")
                if attempt < max_retries - 1:
                    await self.send_update(stage_name, "active", 0, f"–û—à–∏–±–∫–∞, –ø–æ–≤—Ç–æ—Ä—è–µ–º —á–µ—Ä–µ–∑ {2 ** (attempt + 1)} —Å–µ–∫...")
                else:
                    await self.send_update(stage_name, "error", 0, f"–û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
        
        # If all retries failed, raise the last exception
        raise last_exception
    
    def clean_report_content(self, content: str) -> str:
        """Clean report content by removing standalone single asterisks"""
        if not content:
            return content
        
        # Remove standalone single asterisks (not part of bold formatting)
        # This regex looks for single asterisks that are not part of **bold** or *italic* formatting
        import re
        cleaned_content = re.sub(r'(?<!\*)\*(?!\*)(?![^*]*\*)', '', content)
        
        # Remove asterisks after colons (like "–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:* –°—Ä–µ–¥–Ω—è—è")
        cleaned_content = re.sub(r':\*\s*', ': ', cleaned_content)
        
        return cleaned_content
    
    async def process_research(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Process research through all stages"""
        try:
            print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞: {research_type}")
            print(f"üìä –î–∞–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {research_data}")
            
            # Stage 1: Data Collection
            print("üì° –≠—Ç–∞–ø 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
            await self.send_update("data_collection", "active", 0, "–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ä—ã–Ω–∫–µ...")
            market_data = await self.collect_market_data(research_data, research_type)
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã: {len(market_data.get('companies', []))} –∫–æ–º–ø–∞–Ω–∏–π")
            
            # Stage 1.5: Local Documents Insights
            print("üìö –≠—Ç–∞–ø 1.5: –õ–æ–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (PDF)")
            await self.send_update("local_documents", "active", 0, "–ò—â–µ–º –∏–Ω—Å–∞–π—Ç—ã –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF...")
            local_insights = await self.collect_local_documents_insights(research_data, research_type)
            market_data["local_insights"] = local_insights or {"insights": [], "files": []}
            print(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã: {len(market_data['local_insights'].get('insights', []))} —Ñ–∞–∫—Ç–æ–≤ –∏–∑ {len(market_data['local_insights'].get('files', []))} —Ñ–∞–π–ª–æ–≤")
            
            # Stage 2: Case Analysis
            print("üîç –≠—Ç–∞–ø 2: –ê–Ω–∞–ª–∏–∑ –∫–µ–π—Å–æ–≤")
            await self.send_update("case_analysis", "active", 0, "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–µ–π—Å—ã...")
            cases = await self.analyze_cases(market_data, research_data, research_type)
            print(f"‚úÖ –ö–µ–π—Å—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {len(cases)} –∫–µ–π—Å–æ–≤")
            
            # Stage 3: Report Generation
            print("üìù –≠—Ç–∞–ø 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞")
            await self.send_update("report_generation", "active", 0, "–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç...")
            report = await self.generate_report(cases, research_data, research_type)
            print(f"‚úÖ –û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤: {len(report)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # Stage 4: Link Verification (Final)
            print("üîó –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫")
            await self.send_update("link_verification", "active", 0, "–ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—Ç—á–µ—Ç–µ...")
            final_report = await self.verify_report_links(report)
            print(f"‚úÖ –í—Å–µ —Å—Å—ã–ª–∫–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã: {len(final_report)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return {
                "success": True,
                "report": final_report,
                "stages_completed": 5
            }
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {str(e)}")
            print(f"üìã –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:\n{error_details}")
            await self.send_update("error", "error", 0, f"–û—à–∏–±–∫–∞: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "error_details": error_details
            }
    
    async def collect_market_data(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Stage 1: Collect market data with retry mechanism"""
        return await self._execute_with_retry(
            self._collect_market_data_internal,
            research_data,
            research_type,
            stage_name="data_collection",
            stage_description="—Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"
        )
    
    async def _collect_market_data_internal(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Internal method for data collection"""
        await self.send_update("data_collection", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å...")
        
        prompt = self.get_data_collection_prompt(research_data, research_type)
        print(f"üìù –ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        await self.send_update("data_collection", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò...")
        
        async with httpx.AsyncClient(timeout=270.0) as client:
            await self.send_update("data_collection", "active", 40, "–í—ã–ø–æ–ª–Ω—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å...")
            api_url = f"{self.config.GEMINI_API_URL}/v1beta/models/{self.config.GEMINI_MODEL}:generateContent"
            print(f"üåê –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ {api_url}")
            
            response = await client.post(
                api_url,
                headers={
                    "Content-Type": "application/json",
                    "x-goog-api-key": self.config.GEMINI_API_KEY
                },
                json={
                    "contents": [{
                        "parts": [{"text": prompt}]
                    }],
                    "generationConfig": {
                        "temperature": 0.7
                    }
                }
            )
            
            print(f"üì° –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: {response.status_code}")
            await self.send_update("data_collection", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç...")
            
            if response.status_code == 200:
                result = response.json()
                print(f"‚úÖ –û—Ç–≤–µ—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω: {len(str(result))} —Å–∏–º–≤–æ–ª–æ–≤")
                await self.send_update("data_collection", "active", 90, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")
                
                # Parse the response to extract structured data
                market_data = self.parse_market_data(result, research_type)
                
                await self.send_update("data_collection", "completed", 100, f"–ù–∞–π–¥–µ–Ω–æ {len(market_data.get('companies', []))} –∫–æ–º–ø–∞–Ω–∏–π")
                
                return market_data
            else:
                error_msg = f"API Error: {response.status_code} - {response.text}"
                print(f"‚ùå {error_msg}")
                await self.send_update("data_collection", "error", 0, error_msg)
                raise Exception(error_msg)

    async def collect_local_documents_insights(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Stage 1.5: Extract and summarize insights from local PDFs with retry"""
        return await self._execute_with_retry(
            self._collect_local_documents_insights_internal,
            research_data,
            research_type,
            stage_name="local_documents",
            stage_description="–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF"
        )

    def _read_pdf_text(self, file_path: str, max_chars: int = 20000) -> str:
        """Extract text from a PDF file with a character cap for efficiency"""
        text_parts: List[str] = []
        try:
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text() or ""
                    if page_text:
                        text_parts.append(page_text)
                    if sum(len(p) for p in text_parts) >= max_chars:
                        break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {file_path}: {e}")
        text = "\n".join(text_parts)
        # Normalize excessive whitespace
        text = re.sub(r"\s+", " ", text)
        return text[:max_chars]

    async def _collect_local_documents_insights_internal(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        await self.send_update("local_documents", "active", 5, "–°–∫–∞–Ω–∏—Ä—É–µ–º –∫–∞—Ç–∞–ª–æ–≥ —Å PDF...")
        data_dir = self.config.DATA_DIR
        pdf_files = []
        try:
            for name in os.listdir(data_dir):
                if name.lower().endswith(".pdf"):
                    pdf_files.append(os.path.join(data_dir, name))
        except Exception as e:
            await self.send_update("local_documents", "error", 0, f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–∞—Ç–∞–ª–æ–≥—É: {e}")
            return {"insights": [], "files": []}

        if not pdf_files:
            await self.send_update("local_documents", "completed", 100, "PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return {"insights": [], "files": []}

        await self.send_update("local_documents", "active", 10, f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(pdf_files)}. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç...")
        files_payload: List[Dict[str, Any]] = []
        total_chars = 0
        
        # Process each PDF file with progress updates
        for i, f in enumerate(pdf_files):
            progress = int((i / len(pdf_files)) * 40) + 10  # 10-50%
            await self.send_update("local_documents", "active", progress, 
                                 f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç {i+1}/{len(pdf_files)}")
            
            print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF {i+1}/{len(pdf_files)}: {os.path.basename(f)}")
            
            text = self._read_pdf_text(f, max_chars=18000)
            total_chars += len(text)
            files_payload.append({
                "file": os.path.basename(f),
                "excerpt": text
            })
            
            print(f"üìä PDF {i+1}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # Small delay to show progress
            await asyncio.sleep(0.2)
        
        await self.send_update("local_documents", "active", 55, f"–¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ {len(files_payload)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        prompt = self.get_local_documents_prompt(files_payload, research_data, research_type)
        await self.send_update("local_documents", "active", 65, "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        async with httpx.AsyncClient(timeout=120.0) as client:
            await self.send_update("local_documents", "active", 70, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò...")
            response = await client.post(
                f"{self.config.GEMINI_API_URL}/v1beta/models/{self.config.GEMINI_MODEL}:generateContent",
                headers={
                    "Content-Type": "application/json",
                    "x-goog-api-key": self.config.GEMINI_API_KEY
                },
                json={
                    "contents": [{
                        "parts": [{"text": prompt}]
                    }],
                    "generationConfig": {
                        "temperature": 0.2
                    }
                }
            )

        if response.status_code != 200:
            await self.send_update("local_documents", "error", 0, f"API Error: {response.status_code}")
            return {"insights": [], "files": [f["file"] for f in files_payload]}

        await self.send_update("local_documents", "active", 85, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç –ò–ò...")
        result = response.json()
        content = result.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
        
        await self.send_update("local_documents", "active", 90, "–ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã...")
        insights = self.parse_local_insights(content)
        
        # Count insights by source file
        insights_by_file = {}
        for insight in insights:
            source_file = insight.get("source_file", "unknown.pdf")
            if source_file not in insights_by_file:
                insights_by_file[source_file] = 0
            insights_by_file[source_file] += 1
        
        # Create summary message without specific file names
        summary = f"–ù–∞–π–¥–µ–Ω–æ {len(insights)} –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ {len(files_payload)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        
        await self.send_update("local_documents", "completed", 100, summary)
        
        print(f"üìà –ò–¢–û–ì–ò –û–ë–†–ê–ë–û–¢–ö–ò PDF:")
        print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(files_payload)}")
        print(f"   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∏–Ω—Å–∞–π—Ç–æ–≤: {len(insights)}")
        for file, count in insights_by_file.items():
            print(f"   {file}: {count} –∏–Ω—Å–∞–π—Ç–æ–≤")
        
        return {"insights": insights, "files": [f["file"] for f in files_payload]}
    
    async def analyze_cases(self, market_data: Dict[str, Any], research_data: Dict[str, Any], research_type: str) -> List[Dict[str, Any]]:
        """Stage 2: Analyze cases with retry mechanism"""
        return await self._execute_with_retry(
            self._analyze_cases_internal,
            market_data,
            research_data,
            research_type,
            stage_name="case_analysis",
            stage_description="–∞–Ω–∞–ª–∏–∑–∞ –∫–µ–π—Å–æ–≤"
        )
    
    async def _analyze_cases_internal(self, market_data: Dict[str, Any], research_data: Dict[str, Any], research_type: str) -> List[Dict[str, Any]]:
        """Internal method for case analysis"""
        await self.send_update("case_analysis", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–µ–π—Å–æ–≤...")
        
        prompt = self.get_case_analysis_prompt(market_data, research_data, research_type)
        
        await self.send_update("case_analysis", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑...")
        
        async with httpx.AsyncClient(timeout=270.0) as client:
            response = await client.post(
                f"{self.config.GEMINI_API_URL}/v1beta/models/{self.config.GEMINI_MODEL}:generateContent",
                headers={
                    "Content-Type": "application/json",
                    "x-goog-api-key": self.config.GEMINI_API_KEY
                },
                json={
                    "contents": [{
                        "parts": [{"text": prompt}]
                    }],
                    "generationConfig": {
                        "temperature": 0.5
                    }
                }
            )
            
            await self.send_update("case_analysis", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞...")
            
            if response.status_code == 200:
                result = response.json()
                await self.send_update("case_analysis", "active", 90, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –∫–µ–π—Å—ã...")
                
                cases = self.parse_cases(result)
                await self.send_update("case_analysis", "completed", 100, f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(cases)} –∫–µ–π—Å–æ–≤")
                
                return cases
            else:
                raise Exception(f"API Error: {response.status_code}")
    
    
    
    
    async def generate_report(self, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Stage 4: Generate final report with retry mechanism"""
        return await self._execute_with_retry(
            self._generate_report_internal,
            cases,
            research_data,
            research_type,
            stage_name="report_generation",
            stage_description="–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"
        )
    
    async def _generate_report_internal(self, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Internal method for report generation"""
        await self.send_update("report_generation", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—á–µ—Ç–∞...")
        
        prompt = self.get_report_generation_prompt(cases, research_data, research_type)
        
        await self.send_update("report_generation", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–∞...")
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{self.config.GEMINI_API_URL}/v1beta/models/{self.config.GEMINI_MODEL}:generateContent",
                headers={
                    "Content-Type": "application/json",
                    "x-goog-api-key": self.config.GEMINI_API_KEY
                },
                json={
                    "contents": [{
                        "parts": [{"text": prompt}]
                    }],
                    "generationConfig": {
                        "temperature": 0.3
                    }
                }
            )
            
            await self.send_update("report_generation", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç...")
            
            if response.status_code == 200:
                result = response.json()
                await self.send_update("report_generation", "active", 90, "–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç...")
                
                report_content = self.extract_report_content(result)
                
                # Enhance report with additional links
                await self.send_update("report_generation", "active", 95, "–î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏...")
                enhanced_report = await self.enhance_report_with_links(report_content, cases, research_data, research_type)
                
                # Clean the report content before final processing
                final_report = self.clean_report_content(enhanced_report)
                
                # Final report length check
                print(f"üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
                print(f"   –î–ª–∏–Ω–∞ –æ—Ç—á–µ—Ç–∞: {len(final_report)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤: {final_report.count(chr(10)) + 1}")
                print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫: {final_report.count('[')}")
                
                await self.send_update("report_generation", "completed", 100, f"–û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤! ({len(final_report)} —Å–∏–º–≤–æ–ª–æ–≤)")
                
                return final_report
            else:
                raise Exception(f"API Error: {response.status_code}")
    
    def get_data_collection_prompt(self, research_data: Dict[str, Any], research_type: str) -> str:
        """Get prompt for data collection stage"""
        if research_type == "feature":
            return f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É –∏ —Å–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –æ —Ñ–∏–Ω—Ç–µ—Ö-–ø—Ä–æ–¥—É–∫—Ç–∞—Ö.

–¶–ï–õ–¨: –ù–∞–π—Ç–∏ –∏ —Å–æ–±—Ä–∞—Ç—å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ü–û–î–†–û–ë–ù–£–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ñ–∏—á—É "{research_data.get('research_element', '')}".

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –≠–ª–µ–º–µ–Ω—Ç: {research_data.get('research_element', '')}
- –ë–µ–Ω—á–º–∞—Ä–∫–∏: {research_data.get('benchmarks', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–≥—Ä–æ–∫–∏: {research_data.get('required_players', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã: {research_data.get('required_countries', '')}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ü–û–ò–°–ö –°–°–´–õ–û–ö:
1. –ù–∞–π–¥–∏ –ú–ò–ù–ò–ú–£–ú 15-20 –∫–æ–º–ø–∞–Ω–∏–π
2. –î–ª—è –ö–ê–ñ–î–û–ô –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞–π–¥–∏ –ú–ò–ù–ò–ú–£–ú 8-10 –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –°–°–´–õ–û–ö:
   - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –∫–æ–º–ø–∞–Ω–∏–∏
   - –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ (LinkedIn, Twitter, Facebook)
   - –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏
   - –ö–µ–π—Å—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –æ—Ç–∑—ã–≤—ã
   - –ü—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑—ã –∏ –Ω–æ–≤–æ—Å—Ç–∏
   - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–∏
   - –ü–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
   - –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∏ –Ω–∞–≥—Ä–∞–¥—ã
   - –ë–ª–æ–≥–∏ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
   - –û—Ç–∑—ã–≤—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–π—Ç–∏–Ω–≥–∏

3. –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ –º–∞–ª–æ - –∏—â–∏ –ì–õ–£–ë–ñ–ï:
   - –ü—Ä–æ–≤–µ—Ä—è–π LinkedIn, Crunchbase, TechCrunch, Product Hunt
   - –ò—â–∏ –≤ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö –∏–∑–¥–∞–Ω–∏—è—Ö, –±–ª–æ–≥–∞—Ö, —Ñ–æ—Ä—É–º–∞—Ö
   - –ü—Ä–æ–≤–µ—Ä—è–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
   - –ï—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–¥–∏—à—å —Å—Å—ã–ª–∫–∏ - –∏—â–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

–í–ê–ñ–ù–û:
- –ù–ï –≥–µ–Ω–µ—Ä–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ù–ï —Å–æ–∑–¥–∞–≤–∞–π —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å—Å—ã–ª–æ–∫
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ —Å—Ç—Ä–∞–Ω–∞–º/—Ä–µ–≥–∏–æ–Ω–∞–º
- –ö–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
  –ö–æ–º–ø–∞–Ω–∏—è: [–Ω–∞–∑–≤–∞–Ω–∏–µ]
  –°–∞–π—Ç: [–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç]
  –°–æ—Ü—Å–µ—Ç–∏: [—Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏]
  –ü—Ä–æ–¥—É–∫—Ç—ã: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã]
  –ö–µ–π—Å—ã: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–µ–π—Å—ã]
  –ù–æ–≤–æ—Å—Ç–∏: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏]
  –§–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ: [—Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ]
  –ü–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞]
  –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è]
  –ë–ª–æ–≥–∏: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –±–ª–æ–≥–∏]
  –û—Ç–∑—ã–≤—ã: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ç–∑—ã–≤—ã]
  –°—Ç—Ä–∞–Ω–∞: [—Å—Ç—Ä–∞–Ω–∞]
  –û–ø–∏—Å–∞–Ω–∏–µ: [–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∏—á–∏]

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π —Å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Å—ã–ª–æ–∫.
"""
        else:  # product research
            return f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É –∏ —Å–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –æ —Ñ–∏–Ω—Ç–µ—Ö-–ø—Ä–æ–¥—É–∫—Ç–∞—Ö.

–¶–ï–õ–¨: –ù–∞–π—Ç–∏ –∏ —Å–æ–±—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ "{research_data.get('product_characteristics', '')}".

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {research_data.get('product_characteristics', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–≥—Ä–æ–∫–∏: {research_data.get('required_players', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã: {research_data.get('required_countries', '')}

–ó–ê–î–ê–ß–ê:
1. –ù–∞–π–¥–∏ 15-20 –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ —É–∫–∞–∂–∏:
   - –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –∏ –∫–æ–º–ø–∞–Ω–∏–∏
   - –°—Ç—Ä–∞–Ω–∞/—Ä–µ–≥–∏–æ–Ω
   - –¢–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞
   - –ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
   - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç
   - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

–í–ê–ñ–ù–û:
- –ù–ï –≥–µ–Ω–µ—Ä–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ù–ï —Å–æ–∑–¥–∞–≤–∞–π —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ö–∞–∂–¥—ã–π –ø—Ä–æ–¥—É–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
  –ü—Ä–æ–¥—É–∫—Ç: [–Ω–∞–∑–≤–∞–Ω–∏–µ]
  –ö–æ–º–ø–∞–Ω–∏—è: [–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏]
  –°–∞–π—Ç: [–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç]
  –°—Ç—Ä–∞–Ω–∞: [—Å—Ç—Ä–∞–Ω–∞]
  –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: [–∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏]

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.
"""
    
    def get_local_documents_prompt(self, files_payload: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Prompt for summarizing local PDF documents into structured insights (JSON)."""
        context = {
            "research_type": research_type,
            "product_description": research_data.get('product_description', ''),
            "segment": research_data.get('segment', ''),
        }
        if research_type == "feature":
            context["focus"] = research_data.get('research_element', '')
        else:
            context["focus"] = research_data.get('product_characteristics', '')

        return (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ª–æ–∫–∞–ª—å–Ω—ã–µ PDF-–¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∏–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é.\n"
            "–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å—Ç—Ä–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–º JSON-–º–∞—Å—Å–∏–≤–µ –æ–±—ä–µ–∫—Ç–æ–≤ –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π. –°—Ö–µ–º–∞ –æ–±—ä–µ–∫—Ç–∞:\n"
            "{\"source_file\": str, \"section\": str, \"fact\": str, \"metrics\": str|null, \"date\": str|null, \"links\": [str]}\n"
            "–ü—Ä–∞–≤–∏–ª–∞: –∫–æ—Ä–æ—Ç–∫–æ, –ø–æ –¥–µ–ª—É; –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –µ—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ; –∏–≥–Ω–æ—Ä–∏—Ä—É–π –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ.\n\n"
            f"–ö–û–ù–¢–ï–ö–°–¢ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø: {json.dumps(context, ensure_ascii=False)}\n\n"
            f"–ò–°–¢–û–ß–ù–ò–ö–ò: {json.dumps(files_payload[:3], ensure_ascii=False) if len(files_payload)>3 else json.dumps(files_payload, ensure_ascii=False)}\n\n"
            "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ JSON –±–µ–∑ –º–∞—Ä–∫–∏—Ä–æ–≤–æ–∫."
        )

    def get_case_analysis_prompt(self, market_data: Dict[str, Any], research_data: Dict[str, Any], research_type: str) -> str:
        """Get prompt for case analysis stage"""
        if research_type == "feature":
            return f"""
–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ —Ñ–∏–Ω—Ç–µ—Ö—É. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã.

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
{json.dumps(market_data, ensure_ascii=False, indent=2)}

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –≠–ª–µ–º–µ–Ω—Ç: {research_data.get('research_element', '')}

–õ–û–ö–ê–õ–¨–ù–´–ï –ò–ù–°–ê–ô–¢–´ –ò–ó PDF:
{json.dumps(market_data.get('local_insights', {}), ensure_ascii=False, indent=2)}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û –°–°–´–õ–û–ö:
1. –î–ª—è –ö–ê–ñ–î–û–ì–û –∫–µ–π—Å–∞ –Ω–∞–π–¥–∏ –ú–ò–ù–ò–ú–£–ú 5-7 –ü–û–î–¢–í–ï–†–ñ–î–ê–Æ–©–ò–• –°–°–´–õ–û–ö
2. –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ –º–∞–ª–æ - –∏—â–∏ –ì–õ–£–ë–ñ–ï –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. –ü—Ä–æ–≤–µ—Ä—è–π –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å
4. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞:
   - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
   - –ö–µ–π—Å—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - –û—Ç–∑—ã–≤—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
   - –ü—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑—ã –∏ –Ω–æ–≤–æ—Å—Ç–∏
   - –ü–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
   - –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∏ –Ω–∞–≥—Ä–∞–¥—ã

–ó–ê–î–ê–ß–ê:
–°–æ–∑–¥–∞–π 10-12 –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∫–µ–π—Å–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É:

**–ö–µ–π—Å [–Ω–æ–º–µ—Ä]: [–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏]**

**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:** [1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è]

**–°—Ç—Ä–∞–Ω–∞:** [—Å—Ç—Ä–∞–Ω–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏]

**–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∏—á–∏:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–∏—á–∞
- –ì–¥–µ –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –ø—É—Ç–∏
- –î–ª—è –∫–æ–≥–æ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞
- –ö–∞–∫–∏–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã
- –ú–µ—Ç—Ä–∏–∫–∏/—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–ú–ò–ù–ò–ú–£–ú 5-7 —Å—Å—ã–ª–æ–∫):**
- [–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ 1]
- [–ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è —Å—Å—ã–ª–∫–∞ 2]
- [–∫–µ–π—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 3]
- [—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è 4]
- [–æ—Ç–∑—ã–≤ –∫–ª–∏–µ–Ω—Ç–∞ 5]
- [–Ω–æ–≤–æ—Å—Ç—å/–ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑ 6]
- [–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ/–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è 7]


–í–ê–ñ–ù–û:
- –ù–ï –≥–µ–Ω–µ—Ä–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ù–ï —Å–æ–∑–¥–∞–≤–∞–π —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
- –ö–∞–∂–¥—ã–π –∫–µ–π—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
- –í—Å–µ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–±–æ—á–∏–º–∏
- –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞—Ç—ã
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ –∫ –Ω–∞—à–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
"""
        else:  # product research
            return f"""
–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ —Ñ–∏–Ω—Ç–µ—Ö—É. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤.

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
{json.dumps(market_data, ensure_ascii=False, indent=2)}

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {research_data.get('product_characteristics', '')}

–õ–û–ö–ê–õ–¨–ù–´–ï –ò–ù–°–ê–ô–¢–´ –ò–ó PDF:
{json.dumps(market_data.get('local_insights', {}), ensure_ascii=False, indent=2)}

–ó–ê–î–ê–ß–ê:
–°–æ–∑–¥–∞–π 10-12 –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∫–µ–π—Å–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É:

**–ö–µ–π—Å [–Ω–æ–º–µ—Ä]: [–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞]**

**–ö–æ–º–ø–∞–Ω–∏—è:** [–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏]

**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:** [1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ]

**–°—Ç—Ä–∞–Ω–∞:** [—Å—Ç—Ä–∞–Ω–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏]

**–ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- [—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ 1]
- [—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ 2]
- [—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ 3]

**–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:**
- –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
- –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
- –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã/–º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**
- [—Å—Å—ã–ª–∫–∞ 1]
- [—Å—Å—ã–ª–∫–∞ 2]
- [—Å—Å—ã–ª–∫–∞ 3]


–í–ê–ñ–ù–û:
- –ù–ï –≥–µ–Ω–µ—Ä–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ù–ï —Å–æ–∑–¥–∞–≤–∞–π —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
- –ö–∞–∂–¥—ã–π –∫–µ–π—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
- –í—Å–µ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–±–æ—á–∏–º–∏
- –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
"""
    
    def get_report_generation_prompt(self, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Get prompt for final report generation"""
        if research_type == "feature":
            return f"""
–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ —Ñ–∏–Ω—Ç–µ—Ö—É. –°–æ–∑–¥–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–µ–π—Å–æ–≤.

–ü–†–û–ê–ù–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ö–ï–ô–°–´:
{json.dumps(cases, ensure_ascii=False, indent=2)}

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –≠–ª–µ–º–µ–Ω—Ç: {research_data.get('research_element', '')}

–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π —Ç–∞–∫–∂–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ PDF-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏ –∏ —Ç—Ä–µ–Ω–¥–∞–º–∏.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –°–°–´–õ–ö–ò –ù–ê –õ–û–ö–ê–õ–¨–ù–´–ï –§–ê–ö–¢–´:
1. –ï—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Ñ–∞–∫—Ç—ã –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF - –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–∏—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
   - [–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–∫—Ç–∞](http://maclay.pro/data/–∏–º—è_—Ñ–∞–π–ª–∞.pdf)
2. –°—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –∏ –≤–µ—Å—Ç–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ PDF-–¥–æ–∫—É–º–µ–Ω—Ç—ã

–°–û–ó–î–ê–ô –û–¢–ß–ï–¢ –í –°–õ–ï–î–£–Æ–©–ï–ú –§–û–†–ú–ê–¢–ï:

# –û—Ç—á–µ—Ç –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —Ñ–∏—á–∏: {research_data.get('research_element', '')}

## Executive Summary
[–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ 5-7 –ø—É–Ω–∫—Ç–æ–≤]

## –ê–Ω–∞–ª–∏–∑ –∫–µ–π—Å–æ–≤
[–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–µ–π—Å–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π]

### –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–µ–π—Å–æ–≤
| ‚Ññ | –ö–æ–º–ø–∞–Ω–∏—è | –°—Ç—Ä–∞–Ω–∞ | –û–ø–∏—Å–∞–Ω–∏–µ —Ñ–∏—á–∏ | –ò—Å—Ç–æ—á–Ω–∏–∫–∏ |
|---|----------|--------|---------------|-----------|-----------------|
| 1 | [–Ω–∞–∑–≤–∞–Ω–∏–µ] | [—Å—Ç—Ä–∞–Ω–∞] | [–æ–ø–∏—Å–∞–Ω–∏–µ] | [—Å—Å—ã–ª–∫–∏] |
| 2 | [–Ω–∞–∑–≤–∞–Ω–∏–µ] | [—Å—Ç—Ä–∞–Ω–∞] | [–æ–ø–∏—Å–∞–Ω–∏–µ] | [—Å—Å—ã–ª–∫–∏] |
| ... | ... | ... | ... | ... |

## –ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ –Ω–∞—à–µ–º—É –ø—Ä–æ–¥—É–∫—Ç—É
[–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏]

## –ü–ª–∞–Ω –≤–Ω–µ–¥—Ä–µ–Ω–∏—è
[–ü–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Å –æ—Ü–µ–Ω–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏]

## –†–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
[–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ —Å–ø–æ—Å–æ–±—ã –∏—Ö –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏]

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û –°–°–´–õ–û–ö:
1. –í –ö–ê–ñ–î–û–ô —Ç–∞–±–ª–∏—Ü–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ú–ò–ù–ò–ú–£–ú 3-5 —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏—é
2. –í—Å–µ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ü–†–û–í–ï–†–ï–ù–ù–´–ú–ò –∏ –ê–ö–¢–£–ê–õ–¨–ù–´–ú–ò
3. –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ –º–∞–ª–æ - –∏—â–∏ –ì–õ–£–ë–ñ–ï –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
4. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞:
   - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã –∫–æ–º–ø–∞–Ω–∏–π
   - –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
   - –ö–µ–π—Å—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
   - –û—Ç–∑—ã–≤—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
   - –ü—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑—ã –∏ –Ω–æ–≤–æ—Å—Ç–∏
   - –ü–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

–í–ê–ñ–ù–û:
- –ù–ï –≥–µ–Ω–µ—Ä–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ù–ï —Å–æ–∑–¥–∞–≤–∞–π —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- –í—Å–µ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏
- –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–¥–∞—É–Ω —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
- –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –≤ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü–µ
"""
        else:  # product research
            return f"""
–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ —Ñ–∏–Ω—Ç–µ—Ö—É. –°–æ–∑–¥–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤.

–ü–†–û–ê–ù–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ü–†–û–î–£–ö–¢–´:
{json.dumps(cases, ensure_ascii=False, indent=2)}

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {research_data.get('product_characteristics', '')}

–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π —Ç–∞–∫–∂–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ PDF-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏ –∏ —Ç—Ä–µ–Ω–¥–∞–º–∏.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –°–°–´–õ–ö–ò –ù–ê –õ–û–ö–ê–õ–¨–ù–´–ï –§–ê–ö–¢–´:
1. –ï—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Ñ–∞–∫—Ç—ã –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF - –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–∏—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
   - [–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–∫—Ç–∞](http://maclay.pro/data/–∏–º—è_—Ñ–∞–π–ª–∞.pdf)
2. –°—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –∏ –≤–µ—Å—Ç–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ PDF-–¥–æ–∫—É–º–µ–Ω—Ç—ã

–°–û–ó–î–ê–ô –û–¢–ß–ï–¢ –í –°–õ–ï–î–£–Æ–©–ï–ú –§–û–†–ú–ê–¢–ï:

# –û—Ç—á–µ—Ç –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –ø—Ä–æ–¥—É–∫—Ç–∞: {research_data.get('product_characteristics', '')}

## Executive Summary
[–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ 5-7 –ø—É–Ω–∫—Ç–æ–≤]

## –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
[–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π]

### –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤
| ‚Ññ | –ü—Ä–æ–¥—É–∫—Ç | –ö–æ–º–ø–∞–Ω–∏—è | –°—Ç—Ä–∞–Ω–∞ | –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ | –ò—Å—Ç–æ—á–Ω–∏–∫–∏ |
|---|---------|----------|--------|----------------|-----------|-----------------|
| 1 | [–Ω–∞–∑–≤–∞–Ω–∏–µ] | [–∫–æ–º–ø–∞–Ω–∏—è] | [—Å—Ç—Ä–∞–Ω–∞] | [—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏] | [—Å—Å—ã–ª–∫–∏] |
| 2 | [–Ω–∞–∑–≤–∞–Ω–∏–µ] | [–∫–æ–º–ø–∞–Ω–∏—è] | [—Å—Ç—Ä–∞–Ω–∞] | [—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏] | [—Å—Å—ã–ª–∫–∏] |
| ... | ... | ... | ... | ... | ... |

## –†—ã–Ω–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
[–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤]

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
[–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—à–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞]

## –ü–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è
[–ü–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞]

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û –°–°–´–õ–û–ö:
1. –í –ö–ê–ñ–î–û–ô —Ç–∞–±–ª–∏—Ü–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ú–ò–ù–ò–ú–£–ú 3-5 —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç
2. –í—Å–µ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ü–†–û–í–ï–†–ï–ù–ù–´–ú–ò –∏ –ê–ö–¢–£–ê–õ–¨–ù–´–ú–ò
3. –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ –º–∞–ª–æ - –∏—â–∏ –ì–õ–£–ë–ñ–ï –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
4. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞:
   - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
   - –ö–µ–π—Å—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
   - –û—Ç–∑—ã–≤—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
   - –ü—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑—ã –∏ –Ω–æ–≤–æ—Å—Ç–∏
   - –ü–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

–í–ê–ñ–ù–û:
- –ù–ï –≥–µ–Ω–µ—Ä–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ù–ï —Å–æ–∑–¥–∞–≤–∞–π —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- –í—Å–µ —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏
- –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–¥–∞—É–Ω —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
- –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –≤ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü–µ
"""

    def parse_local_insights(self, content: str) -> List[Dict[str, Any]]:
        """Parse JSON array of insights from model response; fallback to simple parsing"""
        content_str = content.strip()
        # Attempt to locate JSON array in the text
        match = re.search(r"\[.*\]", content_str, re.DOTALL)
        json_str = match.group(0) if match else content_str
        try:
            data = json.loads(json_str)
            if isinstance(data, list):
                # Normalize objects
                norm = []
                for item in data:
                    if not isinstance(item, dict):
                        continue
                    source_file = item.get("source_file") or item.get("source") or "unknown.pdf"
                    # Create download link for the PDF file
                    download_link = f"http://maclay.pro/data/{source_file}"
                    norm.append({
                        "source_file": source_file,
                        "download_link": download_link,
                        "section": item.get("section") or "",
                        "fact": item.get("fact") or "",
                        "metrics": item.get("metrics") or None,
                        "date": item.get("date") or None,
                        "links": item.get("links") or []
                    })
                return norm
        except Exception:
            pass
        # Fallback: extract lines starting with '-' or '*'
        insights: List[Dict[str, Any]] = []
        for line in content_str.split('\n'):
            line = line.strip(" -‚Ä¢*")
            if not line:
                continue
            insights.append({
                "source_file": "unknown.pdf",
                "download_link": "http://maclay.pro/data/unknown.pdf",
                "section": "",
                "fact": line,
                "metrics": None,
                "date": None,
                "links": []
            })
        return insights
    
    def parse_market_data(self, api_response: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Parse market data from API response"""
        try:
            if "candidates" in api_response and len(api_response["candidates"]) > 0:
                content = api_response["candidates"][0]["content"]["parts"][0]["text"]
                
                # Parse the content to extract structured data
                companies = self.extract_companies_from_text(content)
                
                return {
                    "raw_content": content,
                    "companies": companies,
                    "research_type": research_type,
                    "timestamp": datetime.now().isoformat(),
                    "total_found": len(companies)
                }
            else:
                return {
                    "raw_content": "No data found",
                    "companies": [],
                    "research_type": research_type,
                    "timestamp": datetime.now().isoformat(),
                    "total_found": 0
                }
        except Exception as e:
            return {
                "raw_content": f"Error parsing data: {str(e)}",
                "companies": [],
                "research_type": research_type,
                "timestamp": datetime.now().isoformat(),
                "total_found": 0
            }
    
    def extract_companies_from_text(self, text: str) -> List[Dict[str, Any]]:
        """Extract company information from text"""
        companies = []
        lines = text.split('\n')
        
        current_company = {}
        for line in lines:
            line = line.strip()
            if not line:
                if current_company:
                    companies.append(current_company)
                    current_company = {}
                continue
                
            # Look for company patterns
            if any(keyword in line.lower() for keyword in ['–∫–æ–º–ø–∞–Ω–∏—è:', 'company:', '–Ω–∞–∑–≤–∞–Ω–∏–µ:', 'name:']):
                if current_company:
                    companies.append(current_company)
                current_company = {"name": line.split(':', 1)[1].strip() if ':' in line else line}
            elif any(keyword in line.lower() for keyword in ['—Å–∞–π—Ç:', 'website:', 'url:']):
                if current_company:
                    current_company["website"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif any(keyword in line.lower() for keyword in ['—Å—Ç—Ä–∞–Ω–∞:', 'country:']):
                if current_company:
                    current_company["country"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif line.startswith('http'):
                if current_company:
                    if "links" not in current_company:
                        current_company["links"] = []
                    current_company["links"].append(line)
        
        if current_company:
            companies.append(current_company)
            
        return companies
    
    def parse_cases(self, api_response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Parse cases from API response"""
        try:
            if "candidates" in api_response and len(api_response["candidates"]) > 0:
                content = api_response["candidates"][0]["content"]["parts"][0]["text"]
                
                # Parse the content to extract case information
                cases = self.extract_cases_from_text(content)
                
                return cases
            else:
                return []
        except Exception as e:
            return []
    
    def extract_cases_from_text(self, text: str) -> List[Dict[str, Any]]:
        """Extract case information from text"""
        cases = []
        lines = text.split('\n')
        
        current_case = {}
        case_number = 1
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Look for case patterns
            if line.startswith(f'**–ö–µ–π—Å {case_number}') or line.startswith(f'–ö–µ–π—Å {case_number}'):
                if current_case:
                    cases.append(current_case)
                current_case = {
                    "number": case_number,
                    "title": line.replace('**', '').replace('*', '').strip()
                }
                case_number += 1
            elif line.startswith('**–ö–æ–º–ø–∞–Ω–∏—è:') or line.startswith('–ö–æ–º–ø–∞–Ω–∏—è:'):
                if current_case:
                    current_case["company"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif line.startswith('**–°—Ç—Ä–∞–Ω–∞:') or line.startswith('–°—Ç—Ä–∞–Ω–∞:'):
                if current_case:
                    current_case["country"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif line.startswith('**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:') or line.startswith('–ò—Å—Ç–æ—á–Ω–∏–∫–∏:'):
                if current_case:
                    current_case["sources"] = []
            elif line.startswith('http'):
                if current_case:
                    if "sources" not in current_case:
                        current_case["sources"] = []
                    current_case["sources"].append(line)
            elif current_case and line and not line.startswith('**'):
                # This is likely description content
                if "description" not in current_case:
                    current_case["description"] = line
                else:
                    current_case["description"] += " " + line
        
        if current_case:
            cases.append(current_case)
            
        return cases
    
    def add_verification_summary(self, report_content: str, cases: List[Dict[str, Any]]) -> str:
        """Add verification summary to the report"""
        total_links = 0
        working_links = 0
        broken_links = 0
        
        for case in cases:
            if "verified_links" in case:
                total_links += len(case["verified_links"])
                working_links += len([link for link in case["verified_links"] if link.get("status") == "working"])
            if "broken_links" in case:
                broken_links += len(case["broken_links"])
        
        percentage = (working_links/total_links*100) if total_links > 0 else 0
        
        verification_summary = f"""

## –°–≤–æ–¥–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫

- **–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ:** {total_links}
- **–†–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {working_links}
- **–ù–µ—Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {broken_links}
- **–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {percentage:.1f}%

*–í—Å–µ —Å—Å—ã–ª–∫–∏ –±—ã–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å.*
"""
        
        return report_content + verification_summary
    
    def extract_report_content(self, api_response: Dict[str, Any]) -> str:
        """Extract report content from API response"""
        try:
            if "candidates" in api_response and len(api_response["candidates"]) > 0:
                content = api_response["candidates"][0]["content"]["parts"][0]["text"]
                return content
            else:
                return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
    
    async def enhance_report_with_links(self, report_content: str, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Enhance report with additional links from verified sources"""
        try:
            # Extract all verified links from cases
            all_verified_links = []
            for case in cases:
                if "verified_links" in case:
                    for link in case["verified_links"]:
                        if link.get("status") == "working":
                            all_verified_links.append({
                                "url": link.get("url"),
                                "company": case.get("title", case.get("company", "Unknown")),
                                "context": case.get("description", "")
                            })
            
            if not all_verified_links:
                return report_content
            
            # Create prompt for link enhancement
            # Use full report content, but limit to reasonable size for API
            max_content_length = 15000  # Increased from 3000
            report_preview = report_content[:max_content_length]
            if len(report_content) > max_content_length:
                report_preview += "\n\n[... –æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –æ—Ç—á–µ—Ç–∞ ...]"
            
            prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç—ã. –£–ª—É—á—à–∏ –æ—Ç—á–µ—Ç, –¥–æ–±–∞–≤–∏–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–û–¢–ß–ï–¢ –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:
{report_preview}

–ü–†–û–í–ï–†–ï–ù–ù–´–ï –°–°–´–õ–ö–ò:
{json.dumps(all_verified_links[:20], ensure_ascii=False, indent=2)}

–ó–ê–î–ê–ß–ê:
1. –ù–∞–π–¥–∏ –≤ –æ—Ç—á–µ—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤
2. –î–æ–±–∞–≤—å –∫ –Ω–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç: [—Ç–µ–∫—Å—Ç](—Å—Å—ã–ª–∫–∞)
4. –ù–ï –∏–∑–º–µ–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç—á–µ—Ç–∞, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏
5. –ú–∞–∫—Å–∏–º—É–º 3-5 —Å—Å—ã–ª–æ–∫ –Ω–∞ –∞–±–∑–∞—Ü
6. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã > –∫–µ–π—Å—ã > –Ω–æ–≤–æ—Å—Ç–∏
7. –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –ü–û–õ–ù–´–ô –æ—Ç—á–µ—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –Ω–µ –æ–±—Ä–µ–∑–∞–π –µ–≥–æ

–í–ï–†–ù–ò –ü–û–õ–ù–´–ô –£–õ–£–ß–®–ï–ù–ù–´–ô –û–¢–ß–ï–¢ –° –î–û–ë–ê–í–õ–ï–ù–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò.
"""
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.config.GEMINI_API_URL}/v1beta/models/{self.config.GEMINI_MODEL}:generateContent",
                    headers={
                        "Content-Type": "application/json",
                        "x-goog-api-key": self.config.GEMINI_API_KEY
                    },
                    json={
                        "contents": [{
                            "parts": [{"text": prompt}]
                        }],
                        "generationConfig": {
                            "temperature": 0.3
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    enhanced_content = result.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    
                    if enhanced_content:
                        print(f"üìä –£–õ–£–ß–®–ï–ù–ò–ï –û–¢–ß–ï–¢–ê:")
                        print(f"   –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {len(report_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        print(f"   –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {len(enhanced_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        return enhanced_content
                    else:
                        print(f"‚ö†Ô∏è –ò–ò –Ω–µ –≤–µ—Ä–Ω—É–ª —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π")
                        return report_content
                else:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {response.status_code}")
                    return report_content
                    
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
            return report_content
    
    async def verify_report_links(self, report_content: str) -> str:
        """Verify all links in the report and remove broken ones"""
        try:
            import re
            
            # Find all markdown links in the report
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            links = re.findall(link_pattern, report_content)
            
            if not links:
                print("üìã –°—Å—ã–ª–∫–∏ –≤ –æ—Ç—á–µ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return report_content
            
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            
            verified_links = []
            broken_links = []
            
            # Check each link
            for i, (text, url) in enumerate(links):
                print(f"üîó –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫—É {i+1}/{len(links)}: {url}")
                
                try:
                    # Skip PDF links to our domain - they should work
                    if url.startswith('http://maclay.pro/data/'):
                        verified_links.append((text, url))
                        print(f"‚úÖ PDF —Å—Å—ã–ª–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞: {url}")
                        continue
                    
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        response = await client.head(url, follow_redirects=True)
                        if response.status_code < 400:
                            verified_links.append((text, url))
                            print(f"‚úÖ –°—Å—ã–ª–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç: {response.status_code}")
                        else:
                            broken_links.append((text, url))
                            print(f"‚ùå –°—Å—ã–ª–∫–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {response.status_code}, —É–¥–∞–ª—è–µ–º")
                            
                except Exception as e:
                    broken_links.append((text, url))
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–∫–∏: {str(e)}, —É–¥–∞–ª—è–µ–º")
            
            # Remove broken links and their text from report
            if broken_links:
                print(f"üóëÔ∏è –£–¥–∞–ª—è–µ–º {len(broken_links)} –Ω–µ—Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º")
                for text, url in broken_links:
                    # Remove the entire link with text completely
                    report_content = report_content.replace(f"[{text}]({url})", "")
                
                # Clean up extra whitespace and empty lines
                import re
                report_content = re.sub(r'\n\s*\n\s*\n', '\n\n', report_content)  # Remove multiple empty lines
                report_content = re.sub(r'^\s*\n', '', report_content, flags=re.MULTILINE)  # Remove empty lines at start
                report_content = report_content.strip()
            
            # Replace original links with verified alternatives
            for text, url in verified_links:
                # Find and replace the original link with the verified one
                original_pattern = f"[{text}]("
                if original_pattern in report_content:
                    # Find the original link and replace it
                    import re
                    pattern = f"\\[{re.escape(text)}\\]\\([^)]+\\)"
                    replacement = f"[{text}]({url})"
                    report_content = re.sub(pattern, replacement, report_content)
            
            print(f"üìä –ò–¢–û–ì–ò –ü–†–û–í–ï–†–ö–ò –°–°–´–õ–û–ö –í –û–¢–ß–ï–¢–ï:")
            print(f"   –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
            print(f"   –†–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫: {len(verified_links)}")
            print(f"   –ù–µ—Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫: {len(broken_links)}")
            if len(links) > 0:
                percentage = (len(verified_links) / len(links)) * 100
                print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–±–æ—á–∏—Ö: {percentage:.1f}%")
            
            return report_content
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç–µ: {str(e)}")
            return report_content
    
